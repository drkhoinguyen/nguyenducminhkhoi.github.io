<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Khoi Nguyen&#39;s Wesbite</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Khoi Nguyen&#39;s Wesbite</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 29 Aug 2025 09:54:41 +0700</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Khoi Nguyen&#39;s Wesbite</title>
      <link>/</link>
    </image>
    
    <item>
      <title>OE3DIS: Open-Ended 3D Point Cloud Instance Segmentation</title>
      <link>/publication/open-ended-3d-instance-segmentation/</link>
      <pubDate>Fri, 29 Aug 2025 09:54:41 +0700</pubDate>
      <guid>/publication/open-ended-3d-instance-segmentation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Supercharged One-step Text-to-Image Diffusion Models with Negative Prompts</title>
      <link>/publication/one-step-negative-prompt-image-generation/</link>
      <pubDate>Sat, 28 Jun 2025 11:21:40 +0700</pubDate>
      <guid>/publication/one-step-negative-prompt-image-generation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models</title>
      <link>/publication/content-style-decomposition/</link>
      <pubDate>Sat, 28 Jun 2025 11:10:48 +0700</pubDate>
      <guid>/publication/content-style-decomposition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion</title>
      <link>/publication/one-step-image-editing/</link>
      <pubDate>Thu, 27 Feb 2025 16:48:52 +0700</pubDate>
      <guid>/publication/one-step-image-editing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Any3DIS: Class-Agnostic 3D Instance Segmentation by 2D Mask Tracking</title>
      <link>/publication/class-agnostic-3d-instance-segmentation/</link>
      <pubDate>Thu, 27 Feb 2025 16:40:25 +0700</pubDate>
      <guid>/publication/class-agnostic-3d-instance-segmentation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SharpDepth: Sharpening Metric Depth Predictions Using Diffusion Distillation</title>
      <link>/publication/sharp-depth/</link>
      <pubDate>Thu, 27 Feb 2025 16:35:19 +0700</pubDate>
      <guid>/publication/sharp-depth/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Selected Computer Vision Tutorials</title>
      <link>/post/cv-tutorials/</link>
      <pubDate>Wed, 11 Dec 2024 11:42:55 +0700</pubDate>
      <guid>/post/cv-tutorials/</guid>
      <description>&lt;p&gt;Computer Vision is a diverse research domain encompassing various topics. This guide compiles essential and up-to-date tutorials, carefully selected from those presented at top-tier conferences in the field, including CVPR, ICCV, ECCV, and SIGGRAPH.&lt;/p&gt;
&lt;p&gt;Here is the link: 
&lt;a href=&#34;https://docs.google.com/document/d/14Meh3cSkDRtVlT2Pcj3TmpQz0TOriGdT_r29mUJa-Sc/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://docs.google.com/document/d/14Meh3cSkDRtVlT2Pcj3TmpQz0TOriGdT_r29mUJa-Sc/edit?usp=sharing&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Semi-supervised 3D Semantic Scene Completion with 2D Vision Foundation Model Guidance</title>
      <link>/publication/3d-semantic-occupancy/</link>
      <pubDate>Wed, 11 Dec 2024 10:54:17 +0700</pubDate>
      <guid>/publication/3d-semantic-occupancy/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models</title>
      <link>/publication/video-try-on/</link>
      <pubDate>Wed, 11 Dec 2024 10:43:57 +0700</pubDate>
      <guid>/publication/video-try-on/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SwiftBrush V2: Make Your One-step Diffusion Model Better Than Its Teacher</title>
      <link>/publication/one-step-text-to-image/</link>
      <pubDate>Thu, 18 Jul 2024 11:57:11 +0700</pubDate>
      <guid>/publication/one-step-text-to-image/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DiverseDream: Diverse Text-to-3D Synthesis with Augmented Text Embedding</title>
      <link>/publication/text-to-3d/</link>
      <pubDate>Sat, 23 Dec 2023 09:55:31 +0700</pubDate>
      <guid>/publication/text-to-3d/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Open3DIS: Open-vocabulary 3D Instance Segmentation with 2D Mask Guidance</title>
      <link>/publication/open-vocab-3d-instance-segmentation/</link>
      <pubDate>Sat, 23 Dec 2023 09:50:43 +0700</pubDate>
      <guid>/publication/open-vocab-3d-instance-segmentation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Stable Messenger: Steganography for Message-Concealed Image Generation</title>
      <link>/publication/steganography/</link>
      <pubDate>Sat, 23 Dec 2023 09:43:55 +0700</pubDate>
      <guid>/publication/steganography/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LP-OVOD: Open-Vocabulary Object Detection by Linear Probing</title>
      <link>/publication/open-vocab-object-detection/</link>
      <pubDate>Fri, 27 Oct 2023 10:20:40 +0700</pubDate>
      <guid>/publication/open-vocab-object-detection/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for Pixel-Level Semantic Segmentation</title>
      <link>/publication/dataset-diffusion/</link>
      <pubDate>Wed, 27 Sep 2023 10:20:49 +0700</pubDate>
      <guid>/publication/dataset-diffusion/</guid>
      <description></description>
    </item>
    
    <item>
      <title>GaPro: Box-Supervised 3D Point Cloud Instance Segmentation Using Gaussian Processes as Pseudo Labelers</title>
      <link>/publication/box-supervised-3d-instance-segmentation/</link>
      <pubDate>Sun, 23 Jul 2023 11:16:37 +0700</pubDate>
      <guid>/publication/box-supervised-3d-instance-segmentation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ProbML</title>
      <link>/post/probml/</link>
      <pubDate>Sun, 23 Jul 2023 10:47:26 +0700</pubDate>
      <guid>/post/probml/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;: Welcome to the reading guide for &amp;ldquo;Probabilistic Machine Learning&amp;rdquo; by Kevin Murphy. The first version of this book, published in 2012 (probml.github.io/pml-book/book0.html) gained widespread recognition in the Machine Learning community and was often referred to as the &amp;ldquo;Bible of Machine Learning.&amp;rdquo; The book&amp;rsquo;s popularity stemmed from its comprehensive coverage of a vast array of Machine Learning topics, providing readers with a deep understanding of the subject. Recognizing the rapidly evolving landscape of Machine Learning, Kevin Murphy has taken the initiative to update and expand the knowledge presented in the original book. As a result, he decided to release a new version of the book, dividing it into two separate volumes with a total of more than 2000 pages. The first book serves as a core introduction to the subject, while the second book delves into more advanced topics.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt;: The knowledge encompassed within these two new books is extensive, making it challenging for beginners to grasp its entirety during their initial reading. As a result, I have taken the initiative to craft this reading guideline. Its purpose is to help filter and select the essential topics from these two books, supplementing them with additional external readings as needed to facilitate a comprehensive understanding. Upon completing these chapters, you will have acquired foundational tools essential for problem-solving in the realms of Machine Learning, Computer Vision, and NLP. This guide will also ensure you do not reinvent the wheel or propose solutions that already exist.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Audience&lt;/strong&gt;: If you&amp;rsquo;re seriously into learning machine learning for your future career, like Ph.D. students, experienced AI/ML practitioners, or researchers, these books are perfect for you. But, if you&amp;rsquo;re an AI engineer looking to learn machine learning, there are other books more suitable for your needs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: It&amp;rsquo;s important to note that the selection of specific chapters or sections in this guidance is subjective and reflects the writer&amp;rsquo;s perspective. The chosen topics may or may not align perfectly with everyone&amp;rsquo;s preferences or needs. As such, it is highly recommended to refer back to the original books and read them in their entirety if necessary.&lt;/p&gt;
&lt;p&gt;Here is the link: 
&lt;a href=&#34;https://tinyurl.com/ProbML&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://tinyurl.com/ProbML&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ISBNet: a 3D Point Cloud Instance Segmentation Network with Instance-aware Sampling and Box-aware Dynamic Convolution,</title>
      <link>/publication/3d-instance-segmentation/</link>
      <pubDate>Wed, 08 Mar 2023 20:27:37 +0700</pubDate>
      <guid>/publication/3d-instance-segmentation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PSENet: Progressive Self-Enhancement Network for Unsupervised Extreme-Light Image Enhancement</title>
      <link>/publication/image-enhancement/</link>
      <pubDate>Tue, 04 Oct 2022 10:31:15 +0700</pubDate>
      <guid>/publication/image-enhancement/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Inductive and Transductive Few-Shot Video Classification via Appearance and Temporal Alignments</title>
      <link>/publication/few-shot-video-classification/</link>
      <pubDate>Mon, 25 Jul 2022 15:21:16 +0700</pubDate>
      <guid>/publication/few-shot-video-classification/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Few Shot Counting and Detection</title>
      <link>/publication/few-shot-counting-detection/</link>
      <pubDate>Mon, 25 Jul 2022 15:03:43 +0700</pubDate>
      <guid>/publication/few-shot-counting-detection/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Geodesic-Former: a Geodesic-Guided Few-shot 3D Point Cloud Instance Segmenter</title>
      <link>/publication/few-shot-3d-instance-segmentation/</link>
      <pubDate>Mon, 25 Jul 2022 14:47:35 +0700</pubDate>
      <guid>/publication/few-shot-3d-instance-segmentation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>iFS-RCNN: An Incremental Few-shot Instance Segmenter</title>
      <link>/publication/incremental-few-shot-instance-segmentation/</link>
      <pubDate>Wed, 30 Mar 2022 08:34:49 +0700</pubDate>
      <guid>/publication/incremental-few-shot-instance-segmentation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PhD Guidance</title>
      <link>/post/phd-guidance/</link>
      <pubDate>Sun, 21 Nov 2021 18:25:31 -0800</pubDate>
      <guid>/post/phd-guidance/</guid>
      <description>&lt;p&gt;This is the PhD guidance from my own experience including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(1) Defining your goals&lt;/li&gt;
&lt;li&gt;(2) How to choose a university and a professor&lt;/li&gt;
&lt;li&gt;(3) The needed mindset&lt;/li&gt;
&lt;li&gt;(4) The worth noting things in research&lt;/li&gt;
&lt;li&gt;(5) Career paths after graduation&lt;/li&gt;
&lt;li&gt;(6) Attitude after graduation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is specially written for Vietnamese student, but you can use Google Translate for your convinence.&lt;/p&gt;
&lt;p&gt;You can find it here: 
&lt;a href=&#34;https://docs.google.com/document/d/1EZwDxa4_201rejxHU5GHG_x1YTnU7z1ztD7FVy3RrKk/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://docs.google.com/document/d/1EZwDxa4_201rejxHU5GHG_x1YTnU7z1ztD7FVy3RrKk/edit?usp=sharing&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>POODLE: Improving Few-shot Learning via Penalizing Out-of-Distribution Samples</title>
      <link>/publication/few-shot-classification/</link>
      <pubDate>Sun, 21 Nov 2021 18:13:10 -0800</pubDate>
      <guid>/publication/few-shot-classification/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Weakly Supervised Amodal Segmenter with Boundary Uncertainty Estimation</title>
      <link>/publication/amodal-instance-segmentation/</link>
      <pubDate>Sun, 21 Nov 2021 17:59:06 -0800</pubDate>
      <guid>/publication/amodal-instance-segmentation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>FAPIS: A Few-shot Anchor-free Part-based Instance Segmenter</title>
      <link>/publication/few-shot-instance-segmentation/</link>
      <pubDate>Sun, 21 Nov 2021 17:44:56 -0800</pubDate>
      <guid>/publication/few-shot-instance-segmentation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Overview of Machine Learning</title>
      <link>/post/overview-ml/</link>
      <pubDate>Sun, 16 Aug 2020 14:04:21 -0700</pubDate>
      <guid>/post/overview-ml/</guid>
      <description>&lt;p&gt;This is my first post on Machine Learning, Deep Learning and Computer Vision series in Medium. I am currently a Ph.D. Student in Computer Science with research interests are Computer Vision and Machine Learning. On this series, I will share with you the roadmap I have experienced. I hope that everything I share is somehow helps you save time when exploring Machine Learning field.&lt;/p&gt;
&lt;p&gt;First, I will discuss about the relationship between Machine Learning and other areas. Artificial Intelligence is a branch of Computer Science which deals with the automation of solving problem. I mean the problems here are not all the problems in the world, just a small part of problems which can be represented as a format that computer can understand. In particular, this is the problem can represented as input and output such as the problem of image classification. The input here is an image and the output is the label of that image. In Artificial Intelligence, there are two kinds of approach, one is to use hard-coded rules and one is to use learnable rules. The approaches that use hard-coded rule or heuristic can be used in solving simple games like Tic-tac-toe (large board version) or Chess. They apply a predefined rule to the input. For example, in chess they evaluate the value of different next-moves based on a human experience. Besides, the approaches that use learnable rules is called Machine Learning. These rules are not pre-defined by an expert but learned from the data itself.&lt;/p&gt;
&lt;p&gt;We cannot say which approach is better, it depends on the data and the expert knowledge you have for that problem. If your problems already have expert knowledge, you can use heuristic approach like in Tic-tac-to or Chess. However, the number of problems that can have an expert knowledge is very small comparing to non-expert knowledge problem. So right now the most active part of Artificial Intelligence is Machine Learning which solving the non-expert knowledge problem. In Machine Learning itself, there are several approaches to solve the non-expert knowledge problem. The input to the problem can have many features, if you know exactly which features are importantly contributing to predicting the final output you can use normal Machine Learning algorithm like SVM, Random Forest or Shallow Neural Network and so on to solve. However, there are a lot of problems that you do not know exactly which features contributing to the prediction to the final output. In these cases, you can use Deep Learning to automatically learn the important features.&lt;/p&gt;
&lt;p&gt;So we can see that, which approaches to use in Artificial Intelligence is depending on the knowledge you know about the problem. There is no best approach for all problems. Next, I will discuss some several kinds of problems in Machine Learning.&lt;/p&gt;
&lt;p&gt;In Machine Learning, we can divide the problems based on the level of supervision of the data. The first one is Supervised Learning. The data you have is in the form of input and output and the output here is exactly what you need. For instance, with image classification problem. The input here is an image of an object and the output is the label of that object. There are two tasks in supervised learning: Classification and Regression. The output in Classification is a discrete value where the output in Regression is a continuous value. For example, given the image of a dog or a cat, you will classify it as dog or cat label. With the regression, we can take the example of predicting the values of a house given the particular features like its position, its current condition and so on.&lt;/p&gt;
&lt;p&gt;The second type of problems in Machine Learning is Unsupervised Learning. In that case, the data is just input and you do not know the output. Normally, we have several problems like Clustering, Dimension Reduction or Structure Learning. And the third type of problems in Machine Learning is between the supervised learning and unsupervised learning. We can have 3 sub-divisions: weakly supervised learning, semi-supervised and reinforcement learning. With weakly supervised learning, the label is not exactly the final output we want but correlated to the final output. For example, in the weakly supervised object localization problem, the final output we want is the object bounding box in the image, but the cost of obtaining these label is very expensive, so instead, we have the label of object in the image like a cat or a dog. As a human, if we know the what kind of object we can easily know the position of that object in the image.&lt;/p&gt;
&lt;p&gt;With semi-supervised learning, we do not have the output of all inputs. The input having corresponding output is just a small number comparing to the inputs that we do not have output because of the cost of obtaining full outputs for all inputs is expensive. Finally, with reinforcement learning we do not have the exact output as well as the highly correlated label. Instead, we just have the signal of doing following action is good or not such as the problem of object localization without the notion of ground-truth bounding box or object label. Here, we just have the signal of recognized correct object in the image. If the bounding box is correct then the score of recognizing object will be higher than incorrect bounding box. That gives us a signal or a hint of finding correct bounding box. Therefore, depending on what types of data you have, you will choose the appropriate machine learning algorithms.
To summarize what we have discussed so far on the first post, you can look at the following diagrams:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/overview-ml/overview.jpeg&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/overview-ml/classify.jpeg&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to use Google Scholar for advancing your research</title>
      <link>/post/google-scholar/</link>
      <pubDate>Sun, 16 Aug 2020 13:50:29 -0700</pubDate>
      <guid>/post/google-scholar/</guid>
      <description>&lt;p&gt;One of the most powerful and common tools for your research is Google Scholar. It contains many useful features that are necessary for your research. If you can make use of it, you do not need any proprietary software. In this article, I will introduce some useful features namely Google Scholar Search, My Profile, My library, Alerts, and Metrics.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/4372/1*a9izr3ZSmj8Lqo-qoHoriw.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The above picture captures the homepage of Google Scholar. From here, you can do many things. Let’s get started with Google Scholar Search.&lt;/p&gt;
&lt;h2 id=&#34;google-scholar-search&#34;&gt;&lt;strong&gt;Google Scholar Search&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This function is exactly like Google Search. You can search for everything related to academic stuff including papers, authors, institutions. Below is an example of searching for “Geoffrey Hinton”&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/4248/1*cL9V7OSlH3X5CKbY7rYNTQ.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;By clicking on the link Geoffrey Hinton, you can view his profile like the below picture. His majors, his papers (title, authors, and proceedings) sorted by a number of citations or by year. His total number of citations by years as well his h-index and i10-index and his common co-authors. Especially, you can follow him by clicking the Follow button to see his most up-to-date publications sent to your email.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/5012/1*UwHFfZgu3h1c4SPWye96-g.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Or by searching for institutions, e.g. “Stanford University”, you can see its top authors ranked by their citations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/4304/1*8U6-s6jEY4CDkeRMDtDrvQ.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Or by searching for a particular paper, e.g. “Dynamic Routing between capsules” you can see its authors, proceeding, the abstraction, the total number of citations (you can see which papers cite this paper by clicking that link) and related articles. You can also see the PDF version by clicking on the PDF link on the right of this paper (if it is available) or you can link your institution free access to non-free papers from Springers or Natures…. (I will introduce later at the end of this article).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/4364/1*FRqTfpTdUAhlheqS8ViA8A.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Notably, you can save this paper to your library by clicking on the star symbol right below the abstract, or you can get many the citation formats for this paper by clicking on the quote symbol, you can use various formats as shown in the following picture.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2084/1*QuRi_Sf6MeCBOypeP3Uj6w.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is the basic stuff you can do with Google Scholar Search. Furthermore, you can create alert related to the keyword you are searching by clicking on the Create alert button on the left-hand side of the page.&lt;/p&gt;
&lt;h2 id=&#34;my-library&#34;&gt;&lt;strong&gt;My Library&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This section can be seen as a reference manager tool (like EndNote or Mendeley). Here, you can organize your library or your saved papers by using tags (one paper can belong to many tags). You can create tags very easily by clicking on the Manage labels… link&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/3372/1*jCW_N26TqkK0ONMDxRDbPg.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;You can manage your labels like creating a new label, delete or edit an existing label like the following image&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/3732/1*Zlgmfna5ytWgcGr1_ukrFw.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Back to My Libraries, you can label the papers individually or collectively by clicking the checkbox on the left of each paper and choose the label symbol as described in the following image.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/3716/1*Y-UYp8y98qzCeZtUUuTKkA.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;One practical example is you create a label for your writing paper, and add all the referent papers to that tag and click the export symbol to get their references in the format you want. That is super convenient when you are preparing the BibTeX file for your latex submission to conferences.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2524/1*q9AO4kEESQuqAV526-kyVw.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;That’s it. You now have a free Reference manager and can access it everywhere you want.&lt;/p&gt;
&lt;h2 id=&#34;alerts&#34;&gt;Alerts&lt;/h2&gt;
&lt;p&gt;This section is a very helpful part. You can create any alerts based on your keywords. Whenever there is an update in the search result, it will automatically update and send it to your email.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/3800/1*W1OLpQywR_sFrUONHamlWg.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;metrics&#34;&gt;Metrics&lt;/h2&gt;
&lt;p&gt;This section can be seen as an exploration part.&lt;/p&gt;
&lt;p&gt;You can find the ranking of top venues in specific areas ranked by their h5-index. Notice the link: 
&lt;a href=&#34;https://scholar.google.com/citations?view_op=top_venues&amp;amp;hl=en&amp;amp;vq=eng_computervisionpatternrecognition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://scholar.google.com/citations?view_op=&lt;strong&gt;top_venues&lt;/strong&gt;&amp;amp;hl=en&amp;amp;vq=eng_computervisionpatternrecognition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/4452/1*6ZsfA5BuDcPaTPqIPtaq2g.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;You can also search for &lt;strong&gt;top authors&lt;/strong&gt; in specific field by using the link 
&lt;a href=&#34;https://scholar.google.com/citations?view_op=search_authors&amp;amp;hl=en&amp;amp;mauthors=label:computer_vision&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://scholar.google.com/citations?view_op=&lt;strong&gt;search_authors&lt;/strong&gt;&amp;amp;hl=en&amp;amp;mauthors=**label:computer_vision&lt;/a&gt;**&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/4640/1*FQAb4kNBcskTxM38wLqqLg.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;my-profile&#34;&gt;My Profile&lt;/h2&gt;
&lt;p&gt;This is the section that you manage your published publications or your personal academic profile. This is super important to your research.&lt;/p&gt;
&lt;h2 id=&#34;settings&#34;&gt;Settings&lt;/h2&gt;
&lt;p&gt;In this section, besides the normal user account setting, you can set &lt;strong&gt;Library Links&lt;/strong&gt; to your Google Scholar account. That means that you can access to non-free papers from Springers, ACM or IEEE directly by searching and adding your school library.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/3940/1*0X99DVyZxnNFDd4xpuoviQ.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In summary, Google Scholar provides you an excellent tool to advance your research. You can search for a particular paper, author, or institute. You can also manage your saved papers like a reference manager. Finally, you can create alerts as well as explore top venues (conferences/journals) and authors in specific fields. I hope you enjoy reading this article. If you like it, give it a clap on the left-hand side. You can share it with your colleagues as well.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Machine Learning</title>
      <link>/post/intro-to-ml/</link>
      <pubDate>Sun, 16 Aug 2020 13:50:29 -0700</pubDate>
      <guid>/post/intro-to-ml/</guid>
      <description>&lt;p&gt;This is the overview of basic and important machine learning models, methods and concepts and theories. I acknowledge all information and knowledge including images, data… I have taken from those two courses: 
&lt;a href=&#34;https://www.coursera.org/learn/machine-learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.coursera.org/learn/machine-learning&lt;/a&gt; and 
&lt;a href=&#34;http://classes.engr.oregonstate.edu/eecs/fall2015/cs534/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://classes.engr.oregonstate.edu/eecs/fall2015/cs534/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Our series comprise of following topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Section 1: Introduction, Linear regression, Generative and Discriminative Model, Perceptron, Logistic Regression, Naive Bayes and Gaussian Discriminant Analysis&lt;/li&gt;
&lt;li&gt;Section 2: Four important Discriminative Models: K-Nearest Neighbors, Support Vector Machine, Decision Tree and Neural Network.&lt;/li&gt;
&lt;li&gt;Section 3: Ensemble Methods (Bagging, Random Forest, Boosting) and Clustering (HAC, KMeans, GMM, Spectral Clustering).&lt;/li&gt;
&lt;li&gt;Section 4: Dimension Reduction, Major problems in Machine Learning, ML libraries and Summaries.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can download the whole article of summarizing Machine Learning at here: 
&lt;a href=&#34;ml-summary.pdf&#34;&gt;ml-summary&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Self-supervised GAN for Unsupervised Few-shot Object Recognition</title>
      <link>/publication/unsupervised-few-shot/</link>
      <pubDate>Sun, 16 Aug 2020 12:43:28 -0700</pubDate>
      <guid>/publication/unsupervised-few-shot/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Feature Weighting and Boosting for Few-Shot Segmentation</title>
      <link>/publication/few-shot-segmentation/</link>
      <pubDate>Sun, 16 Aug 2020 12:07:50 -0700</pubDate>
      <guid>/publication/few-shot-segmentation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>News</title>
      <link>/news/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      <guid>/news/</guid>
      <description>








&lt;p&gt;&lt;strong&gt;[06/25]&lt;/strong&gt; Two papers accepted to ICCV 2025: CSD-VAR, NASA. Cheers!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[02/25]&lt;/strong&gt; Three papers accepted to CVPR 2025: Any3DIS, SwiftEdit, SharpDepth. Cheers!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[12/24]&lt;/strong&gt; Two papers on 3D Scene Completition and Video Virtual Try-on are accepted to AAAI 2025. Congrats to all authors!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[07/24]&lt;/strong&gt; Two papers (DiverseDream and SwiftBrushv2) accepted to ECCV 2024. Cheers!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[02/24]&lt;/strong&gt; Our Open3DIS is accepted to CVPR 2024. Cheers!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[10/23]&lt;/strong&gt; One paper accepted to WACV 2024 in Open-vocab Object Detection. Congrats to the first authors Chau Pham and Truong Vu for their first paper!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[09/23]&lt;/strong&gt; One paper accepted to NeurIPS 2023. Congrats to the first authors Quang Nguyen and Truong Vu for their first paper!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[07/23]&lt;/strong&gt; One paper accepted to ICCV 2023. Congrats to the first author Tuan Ngo for his third paper!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[03/23]&lt;/strong&gt; One paper accepted to CVPR 2023. Cheers!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[01/23]&lt;/strong&gt; Our paper accepted to WACV 2023 is awarded as the Best Paper - Honorable Mention!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[09/22]&lt;/strong&gt; One paper accepted to WACV 2023. Congrats to Hue - the first author, you are deserved!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[08/22]&lt;/strong&gt; Three papers accepted to ECCV 2022. Congrats to all the authors!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[04/22]&lt;/strong&gt; One paper accepted to CVPR 2022. Cheers!&lt;/p&gt;

</description>
    </item>
    
  </channel>
</rss>
